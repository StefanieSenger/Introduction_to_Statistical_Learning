{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Linear Regression: Exercises\n",
    "My solutions for the exercises in chapter 3.\n",
    "\n",
    "### Conceptual\n",
    "\n",
    "#### 1. \n",
    "Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these\n",
    "p-values. Your explanation should be phrased in terms of `sales`, `TV`, `radio` , and `newspaper` , rather than in terms of the coefficients of the linear model.\n",
    "\n",
    "This is Table 3.4:\n",
    "\n",
    "|               | Coefficient | Std. Error | t-Statistic | p-Value   |\n",
    "|---------------|-------------|------------|-------------|-----------|\n",
    "| Intercept     | 2.939       | 0.3119     | 9.42        | < 0.0001  |\n",
    "| `TV`          | 0.046       | 0.0014     | 32.81       | < 0.0001  |\n",
    "| `radio`       | 0.189       | 0.0086     | 21.89       | < 0.0001  |\n",
    "| `newspaper`   | -0.001      | 0.0059     | -0.18       | 0.8599    |\n",
    "\n",
    "- The p-values each individually (without taking synergy into account) respond to the null hypothesis that the corresponding predictor has no predicting power on the target, the `sales`.\n",
    "- For instance, the low p-value of < 0.0001 for `TV` answers to the null hypothesis, that TV advertisement is not a predictive factor for how high the `sales` are. Because the p-values is extremely low, is is very unlikely that we have found the evidence by chance. So we have to reject the null hypothesis. Same argument for `radio`.\n",
    "- On the other hand, the p-value of 0.8599 for the `newspaper` feature is very high. It is corresponding to the null hypothesis, that newspaper advertisement is not a predictive factor for how high the `sales` are. And since we have found out that it is very likely to get results like this by pure chance, using the p-value, we fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Carefully explain the differences between the KNN classifier and KNN regression methods.\n",
    "\n",
    "- Both methods start the same way by remembering all the data $X$ passed to the model in fit() (at least in scikit-learn that happens during fit; statsmodels accepts X and y during the init already).\n",
    "\n",
    "- When we pass new data for prediction, both methods would access the K nearest neighbours of this data using some defaults or user-passed distance method.\n",
    "\n",
    "- KNN classifier then by default counts the mayority class of the K nearest neighbours and returns it as a prediction for the new data point.\n",
    "\n",
    "- KNN regression by default averages the predictions of the K nearest neighbours and returns it as a prediction for the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Suppose we have a data set with five predictors, $X_1 = GPA$, $X_2 = IQ$, $X_3 = Level$ (1 for College and 0 for High School), $X_4 =$ Interaction between GPA and IQ, and $X_5 =$ Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\\hat{\\beta}_0 = 50, \\hat{\\beta}_1 = 20, \\hat{\\beta}_2 = 0.07, \\hat{\\beta}_3 = 35, \\hat{\\beta}_4 = 0.01, \\hat{\\beta}_5 = -10$.\n",
    "\n",
    "(a) Which answer is correct, and why?\n",
    "\n",
    "- i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\n",
    "- ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\n",
    "- iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\n",
    "- iv. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.\n",
    "\n",
    "    - Answer ii. is correct, because $\\hat{\\beta}_3$ shows a that for college graduates the entry level salary is on average 35.000 US-Dollar higher than for highschool graduates if we fix GPA and IQ, at least if we assume that our sample we trained on is large and random enough to make such predictions (which it is most likely not). Without the assumption, we can still say that in our data, college graduates earn more on average than high school graduates.\n",
    "    - Answer iv. in particular is incorrect, because $\\hat{\\beta}_5$ modifies the influence on GPA in starting salary for college students so that GPA is less a factor for the hight of their starting salary.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = 50 + 4*20 + 110*0.07 + 1*35 + (4*110)*0.01 + (1*4)*-10\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n",
    "\n",
    "- False. How much evidence there is for an effect and the size of an effect are two separate things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "I collect a set of data ($n$ = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = β_0 + β_1X + β_2X^2 + β_3X^3 + \\epsilon$.\n",
    "\n",
    "(a) Suppose that the true relationship between $X$ and $Y$ is linear, i.e. $Y = β_0 + β_1X + \\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "- If we consider only the training RSS, there is a good chance that a high degree polynomial will be able to fit well to all the samples, especially since we only have 100 of them. Any of the scenarios can happen, depending on the variance of the irreducible error. If it is low, I would expect the linear regression model to outperform the polynomial model on the training data. If it is high, probably, the polynomial model will find a better fit (over fitting, since we know the true relationship is linear) on the training data than the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Answer (a) using test rather than training RSS.\n",
    "\n",
    "- On the test data, the linear model will have a lower RSS, since the true data has a linear relationship as well and the polynomial model with degree 3 will have over fit on the training data and thus have a lot of bias on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the\n",
    "other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "- In this case, I would expect the polynomial model to have a lower RSS, because it can bend (fit itself) better to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Answer (c) using test rather than training RSS.\n",
    "\n",
    "- That cannot be answered, it depends on how much the true relationship differs from linear and on many other factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $i$ th fitted value takes the form </br>\n",
    "$\\hat{y}_i = x_i \\hat{\\beta}$,\n",
    "\n",
    "where $\\hat{\\beta} = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i'=1}^{n} x_{i'}^2}$.\n",
    "\n",
    "Show that we can write $\\hat{y}_i = \\sum_{i'=1}^{n} a_{i'} y_{i'}$.\n",
    "\n",
    "What is $a_{i^′}$?\n",
    "\n",
    "Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After spending half a day on this in frustration this is the [best explanation](https://rpubs.com/toleafar/219579) I found.\n",
    "\n",
    "- It seems I should practice manipulating formulas containing summation symbols (which I am doing [here](https://github.com/StefanieSenger/mathy_coding/blob/main/summations.ipynb)). I couldn't find my own solution and I still cannot understand why what [most other solutions did](https://www.lackos.xyz/itsl/Chapter3/conceptual) is allowed.\n",
    "\n",
    "- The formula for $\\hat{\\beta}$ was given in section 3.1.1, in the part \"Optimization method: normal equation\". The normal equation $X^TX\\hat{\\beta}​=X^Ty$ can also be written $\\left( \\sum_{i=1}^{n} x_i^2 \\right) \\hat{\\beta} = \\sum_{i=1}^{n} x_i y_i$ and solving for $\\hat{\\beta}$ gives the formula above.\n",
    "\n",
    "- Also, since the $i^′$ thing was not used in it, but is used here, I have found out that $i^′$ is just to say that this is another index than the other $i$. It is a helper because we need it later.\n",
    "\n",
    "- $a_{i^′}$ are coefficients ...  ¯\\\\\\_(ツ)_/¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
