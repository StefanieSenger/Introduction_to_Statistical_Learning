{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Linear Regression\n",
    "- we want to determine each feature's contribution to the target (if existing)\n",
    "- and we also need to determine the accuracy of our prediction\n",
    "- observe if there is an interaction effect between the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predicting a quantitative response $Y$ on the basis of a single predictor variable $X$\n",
    "- we assume a linear relationship between predictor and target: $Y ≈ β_0 + β_1 X$\n",
    "    - where $β_0$ and $β_1$ are two unknown constants that represent the *intercept* and *slope* terms; together they are known as the models *coefficients* or *parameters*\n",
    "    \n",
    "- we want to produce estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, so that $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$\n",
    "    - where $\\hat{y}$ indicates a prediction of $Y$ on the basis of $X = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Estimating the Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the real $β_0$ and $β_1$ are unknown\n",
    "- we want to find an intercept $\\hat{\\beta}_0$ and a slope $\\hat{\\beta}_1$ such that the resulting line is as close as possible to the $n$ data points\n",
    "\n",
    "#### Cost function \n",
    "- most common way to measure that closeness is **least squares**, which relates to the MSE\n",
    "\n",
    "- let $\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ be the prediction for $Y$ based on the ith value of $X$:\n",
    "    - then $e_i = y_i − \\hat{y_i}$ represents the ith *residual* (difference between `y` and `y_pred` at the ith observation)\n",
    "    - and the **residual sum of squares** (RSS) is the sum of all the squared residuals: $RSS = e_1^2 + e_2^2 + \\cdots + e_n^2$\n",
    "\n",
    "        - $RSS = (y_1 − \\hat{\\beta}_0 − \\hat{\\beta}_1 x_1 )^2 + (y 2 − \\hat{\\beta}_0 − \\hat{\\beta}_1 x_2 )^2 + · · · + (y_n − \\hat{\\beta}_0 − \\hat{\\beta}_1 x_n )^2 $\n",
    "\n",
    "        - we square the residuals to avoid cancellation, we want to stress larger errors and because it creates a smooth, continuous function, which allows us to use calculus-based optimization techniques, such as taking derivatives, to find the minimum of the RSS efficiently\n",
    "        \n",
    "            - side note: in contrast, the *Mean Absolute Error* (MAE) is not continuous and we cannot apply a calculus based optimization (such as deriving the normal equation, because it's not fully differentiable); it produces a median-based regression line rather than a mean-based one\n",
    "    \n",
    "    - The MSE (**Mean Squared Error**) uses the RSS (**Residual Sum of Squares**) for calculating the score over all the `y_true` - `y_pred` - pairs\n",
    "\n",
    "        - the RSS is the raw sum of squared residuals, which measures the total error across all data points: </br>\n",
    "\n",
    "            $RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ </br>\n",
    "            (loss function)\n",
    "\n",
    "        - the MSE normalizes the RSS by dividing by the number of data points ($n$); it represents the average squared error per data point:</br>\n",
    "        \n",
    "            $MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ </br>\n",
    "            (cost function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization method: normal equation\n",
    "- the **normal equations** provide a closed-form solution to optimization problems, unlike iterative optimization algorithms such as gradient descent\n",
    "\n",
    "    - they are derived directly from the RSS which can also be written as $\\text{RSS}(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\| y - X\\beta \\|^2 = (y−Xβ)^T(y−Xβ)$\n",
    "\n",
    "        - where middle term denotes the squared Euclidean norm and can also be written in matrix form, which is the last term\n",
    "\n",
    "- normal equations are one equation per coefficient and there is a matrix representation that sums them all up: </br>\n",
    "\n",
    "$$\n",
    "y = X \\hat{\\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^T y = X^T X \\hat{\\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}  = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "- we can calculate our $\\beta$ as follows:\n",
    "\n",
    "    $\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2} = $\n",
    "            $\\frac{x_i^T y}{x^T x} = (x^T x)^{-1} x^T y$\n",
    "\n",
    "    $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$\n",
    "\n",
    "    - where $\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} y_i$ and $\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} x_i$ are the sample means\n",
    "        - the tree vertical bars stands for the \"equivalent\" symbol, used to indicate that two expressions are defined to be equal in a specific context\n",
    "        - the summation in the numerator of the first formula applies to the entire product\n",
    "        \n",
    "    - the first equation calculates the slope by re-centering the points to their respective means, removing the offset (intercept), so that we can calculate how changes in $x$ correspond to changes in $y$; this is called the [covariance](https://en.wikipedia.org/wiki/Covariance) (see section on the $R²$ statistic below)\n",
    "\n",
    "    - the second equation describes a transition and can be solved by substituting the first one\n",
    "\n",
    "- this is a closed form solution to find the values of $\\hat{\\beta}_0$​ and $\\hat{\\beta}_1$​ that minimize the loss function (RSS)\n",
    "\n",
    "    -  the loss function and the optimization are so closely related here because the goal of optimization is to minimize the loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ISL states, that the normal equations are derived from the RSS by taking partial derivatives with respect to the coefficients $\\hat{\\beta}_0$​ and $\\hat{\\beta}_1$ and setting them equal to zero (while x and y are all fixed)\n",
    "\n",
    "    - however, I cannot see where the derivative taking happens, since everything seems to be explainable algebraically ... (*scratching head*)\n",
    "\n",
    "    - still, I have found some information to build an intuition on how this works (while not being sure if it really applies to OLS):\n",
    "\n",
    "        - taking derivative of a function represents its rate of change\n",
    "\n",
    "        - setting the derivative of the RSS with respect to $\\beta$ equal to $0$ corresponds to finding the stationary points, which may correspond to the minimum (or maximum)\n",
    "\n",
    "        - RSS function is a convex function (a parabola-shaped curve) and there is only one global minimum\n",
    "    \n",
    "- YouTube playlist that [visualises OLS with normal equation](https://www.youtube.com/watch?v=3g-e2aiRfbU&list=PLjgDp12yUmpw7lsyCKzh11ppUFJfzOjfY) and the above formulas\n",
    "    - the videos show a linear algebra take on using the normal equations and seems that these are essentially two perspectives on the same process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimizing via the normal equation is based on the assumption that $X^TX$ is invertible, meaning, $X$ needs to have full rank, meaning no multicollinearity among features\n",
    "    - in this case the normal equation yields an exact solution\n",
    "\n",
    "    - if there is multicollinearity between features ($X$ is not full rank), the normal equation doesn't have a solution and we need to use other optimization techniques\n",
    "\n",
    "- we can find a best-fit line using OLS, even though we know we have an irreducible error, because it gives us the best approximation to the true relationship between the predictors $x$ and the response $y$, by minimizing the total squared error (RSS) across all data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supplemantary Insights: loss function, objective function, cost function\n",
    "\n",
    "- **loss function**: applies to an individual observation\n",
    "    - RSS however is an aggregate loss function (a sum is used as a summary statistic); but it is simply a summed version of the individual squared losses and thus can serve as a loss function here\n",
    "    - in most supervised learning models in scikit-learn, the loss function is aggregated across all data points because these models aim to minimize an overall prediction error (aggregated loss)\n",
    "\n",
    "- **objective function**: aggregates the loss over all observations and might include additional terms (e.g., regularization)\n",
    "\n",
    "    - **cost function**: specific type of objective function that represents the aggregated loss over the entire dataset, but does typically not include other terms like the regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LinearRegression` in scikit-learn\n",
    "\n",
    "- in scikit-learn a `LinearRegression()` model always uses least squares (MSE) as the loss function and does not provide an option to directly choose another loss function like MAE or Huber loss\n",
    "    - for Huber loss, we could use `HuberRegressor()` and for a quantile loss the `QuantileRegressor()`\n",
    "    - a way to use MAE as a loss in scikit-learn would be to use `QuantileRegressor(quantile=0.5)`\n",
    "\n",
    "- the optimisation for a `LinearRegression()` model is a normal equation based on OSL, using the scipys `lsqr` (`scipy.sparse.linalg.lsqr`) that's like OSL, but with some optimisations (it approximates the lowest values rather than calculating it to get around the bottle neck of converting $X^TX$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Assessing the Accuracy of the Coefficient Estimates\n",
    "\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "                                                                                      \n",
    "This part is much longer than the book, because I had to learn basic statistics for understanding it. Therefore, here I describe everything very thoroughly.             \n",
    "                                                                                     \n",
    "Helpful introductory statistics material:                                                                 \n",
    "Podcast: [Statistics Made Simple](https://chartable.com/podcasts/statistics-for-the-social-sciences) </br>\n",
    "[Statistics Foundations](https://www.linkedin.com/learning/statistics-foundations-3-using-data-sets/discover-samples-confidence-intervals-and-hypothesis-testing?u=72605090) Course on LinkedinLearning </br>\n",
    "\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "- the model is now trained, but before we should use it, we need to quantify the uncertainty of the estimated parameters ($\\hat{\\beta}_0$ and $\\hat{\\beta}_1$) by calculating **standard error** for getting **confidence intervals** and by **hypothesis testing** for getting **p-values** \n",
    "\n",
    "    - this is about assessing the precision (confidence we have in our prediction) and significance (from hypothesis testing) of the coefficients before interpreting them\n",
    "\n",
    "- our estimates for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ will most likely not be exactly equal to ${\\beta}_0$ and ${\\beta}_1$, but they should not systematically over- or under-estimate the true parameters: if we could average the estimates obtained over a huge number of data sets, then the average of these estimates should converge to the true parameters\n",
    "\n",
    "    - this property of a model is called unbiased-ness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is comparable to estimating the population mean $\\mu$ from a sample mean $\\bar{x}$ from introductory statistics courses:\n",
    "    - the estimate is then denoted with $\\hat{\\mu}$\n",
    "    - our estimate is a statistic (single value calculated from the data)\n",
    "\n",
    "- to measure how **precise** our estimate is, we compute the **standard error (SE)**\n",
    "\n",
    "    - the standard error quantifies the average amount by which the estimate varies if we repeatedly sample from the population\n",
    "        - it's calculated the same way as the standard deviation in single sample, but here we are talking about the standard error a [sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution), which can be derived empirically from a single sample for instance by [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))\n",
    "\n",
    "    - smaller standard errors indicate greater confidence in the estimate\n",
    "\n",
    "    - e.g. the standard error for $\\hat{\\mu}$ would be calculated </br>\n",
    "    \n",
    "        $\\text{Var}(\\hat{\\mu}) = SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "        - where $\\sigma$ is the standard deviation of each of the realizations from the distribution, $\\sigma^2$ is the variance of the population distribution (measures the spread or variability of the actual data points around the mean) and $n$ is the sample size\n",
    "\n",
    "            - \"sample size\" $n$ here means: how often we have re-sampled the data or number of folds, and not how many samples we have per fold (thinking about cross validation)\n",
    "            \n",
    "            - in order to guess the population distribution while only the sample distribution is known, we assume a distribution (for instance normal distribution)\n",
    "\n",
    "        - the standard error is the square root of the variance\n",
    "\n",
    "        - as sample size increases, we have more data to reliably estimate the mean, reducing the variability (and the standard error) of the sample mean $\\hat{\\mu}$ \n",
    "\n",
    "        - in general, $\\text{Variance}(\\text{something}) = \\frac{\\text{Sum of Squares}}{\\text{Number of those things}} = \\text{Average Sum of Squares}$ and the standard deviation is the square root of that number and is in the same unit as the data (or the statistic we are interested in)\n",
    "\n",
    "- note that the relationship between $\\mu$ and $\\hat{\\mu}$ is unbiased, because we don't deal with an error: would the sample since be large enough, we would be able to perfectly predict from $\\mu$ from $\\hat{\\mu}$\n",
    "    - we also wish to have this unbiased-ness for predictions in models, but we only have it if the error really is normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similarly, the predicted coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ follow a distribution as well, because they are statistics derived from the data (which are randomly samples from a population as well)\n",
    "\n",
    "- our coefficients ($\\hat{\\beta}$) are point estimates from a larger distribution that is a normal distribution centered around their true values ${\\beta}_0$ and ${\\beta}_1$ as long as the sample size we have trained on is large enough ([central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) applies)\n",
    "\n",
    "    - the variance of these coefficients depends on the design matrix $X$ (or $x_i$ in simple regression) and the sample size $n$\n",
    "        - because the design matrix defines how spread out the data is and the more spread out, the more reliably we can estimate the true coefficients\n",
    "        - the more data points $n$ we have, the easier it is to figure out the true line, because there is less variance\n",
    "\n",
    "    - side note: hopefully also our irreducible error $\\epsilon$ is normally distributed with mean $0$ and standard deviation $\\sigma$ (we assume this); there is a connection between the normal distribution of the coefficients and the irreducible error, because the coefficients are linear combinations of the dependent variables and the errors  (which are included in our estimates, because we cannot separate them)\n",
    "        - this assumption is critical for calculating the coefficients' variance analytically (see below)\n",
    "\n",
    "- there are multiple  ways to calculate the **standard error (SE)** for an estimate\n",
    "\n",
    "    - a traditional approach (ISL uses it) is to calculate the standard error analytically\n",
    "\n",
    "    - but we can also empirically estimate it by sampling from the estimates (for instance by [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))) and form a [sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution) from which we can calculate the standard error\n",
    "\n",
    "    - these are the (analytical) formulas for the standard error (SE) for the estimated coefficients (in simple linear regression) that ISL presents us with: </br>\n",
    "\n",
    "        $\\text{SE}(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right]$ </br>\n",
    "            - tells us how much we should expect our guess for $\\hat{\\beta}_0$ to vary from the real value $\\beta_0$ </br>\n",
    "            - $\\text{SE}(\\hat{\\beta}_0)$ would be the same as $\\text{SE}(\\hat{\\mu})$ if $\\bar{x}$ were zero (in which case $\\hat{\\beta}_0$ would be equal to $\\bar{y}$)\n",
    "\n",
    "        $\\text{SE}(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$ </br>\n",
    "            - looks at how the points are spread out in the x-direction: the more spread out the points are, the more we trust our guess for the steepness of the line\n",
    "\n",
    "        - where $\\bar{x}$ represents the mean of the predictors $x_i$​, and ${\\sum_{i=1}^n (x_i - \\bar{x})^2}$ measures the spread (variance) of the predictors\n",
    "\n",
    "        - and where $\\sigma^2 = \\text{Var}(\\epsilon)$\n",
    "            - question: \"What's $\\epsilon$ gotta do with it? What's $\\epsilon$ but a second hand variable?\"\n",
    "\n",
    "                - attempt of answering that question: since it represents the irreducible error or the noise in the data, $\\text{Var}(\\epsilon)$ tells us how spread out or variable this error is: if the variance is small, it means the errors are small and the model predictions are generally close to the actual data points; if the variance is large, it means the errors are large and the predictions are less reliable\n",
    "\n",
    "                - the variance in the data has two components: the variance explained by the model and the variance from the irreducible error ($\\epsilon$); reducing the overall error variance is the goal (while we can only really reduce the residual error variance)\n",
    "\n",
    "        - these formulas are based on several assumptions:\n",
    "\n",
    "            - relationship between $X$ and $y$ is linear\n",
    "\n",
    "            - normal distribution of $\\epsilon$\n",
    "\n",
    "            - error is not correlated with any (other) feature\n",
    "\n",
    "            - error's variance is constant across all observations (homoscedasticity)\n",
    "\n",
    "                - if not all the conditions are met, this formula is still a good approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- standard errors can be used to compute **confidence intervals** around our estimate:\n",
    "\n",
    "    - for instance two standard errors up and down from it, 95% of our data will lie\n",
    "\n",
    "- standard errors can also be used to perform hypothesis tests on the **hypothesis testing**:\n",
    "\n",
    "    - one way for hypothesis testing would be randomized sampling as in `sklearn.model_selection.permutation_test_score()`, where the correlating feature (in this case the target) is randomly shuffled\n",
    "\n",
    "    - ISL suggests another method that involves calculating the standard error analytically and then (assuming normal distribution) we can know which percentile rank a certain value falls onto\n",
    "\n",
    "    - Null Hypothesis is: $H_0: \\beta_1 = 0$ (\"There is no relationship between $X$ and $Y$.\")\n",
    "\n",
    "    - Alternative Hypothesis: $H_a: \\beta_1 \\neq 0$ (\"There is some relationship between $X$ and $Y$.\")\n",
    "\n",
    "    - to test the null hypothesis, we need to determine whether $\\hat{\\beta}_1$ , our estimate for $\\beta_1$ , is sufficiently far from zero that we can be confident that $\\beta_1$ is non-zero\n",
    "\n",
    "        - If $\\text{SE}(\\hat{\\beta}_1)$ is small, then even relatively small values of $\\hat{\\beta}_1$ may provide strong evidence that $\\beta_1 \\neq 0$, and hence that there is a relationship between $X$ and $Y$.\n",
    "        - If $\\text{SE}(\\hat{\\beta}_1)$ is large, then $\\hat{\\beta}_1$ must be large in absolute value in order for us to reject the null hypothesis. \n",
    "\n",
    "    - in practice, we use the *t-statistic*, which measures the number of standard deviations that $\\hat{\\beta}_1$ is away from 0: </br>\n",
    "\n",
    "        $t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}$\n",
    "\n",
    "        - it's like he z-score, but not for normal distributions but for t-distributions instead and it applies better to small sample sizes than to large ones (like the z-score)\n",
    "        - the more degrees of freedom, the more the t-distribution converges to the standard normal distribution\n",
    "\n",
    "- from the t value, we can compute the probability of observing any number equal o |t| or larger: the **p-value**\n",
    "\n",
    "    - a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance: then we reject the null hypothesis\n",
    "\n",
    "- the **significance** of such a hypothesis test is a normative decision (yes or no): we had defined a p-value for which we would reject the null hypothesis and for which we would fail to reject it a priori and the significance of the test is the decision of whether our estimates are passing or failing this test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Assessing the Accuracy of the Model\n",
    "- StatQuest [video on Linear Regression](https://www.youtube.com/watch?v=7ArmBVF2dCs) explains very nicely, what $R²$ is and how to calculate if it's statistically significant using the *p-value*\n",
    "\n",
    "- after we have made a significance test for our coefficients (for instance using `sklearn.model_selection.permutation_test_score()`), we want to measure the extend to which our model fits the data\n",
    "\n",
    "- in linear regression two related quantities are used for that end: the **residual standard error** (RSE) and the $R²$ statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Standard Error\n",
    "\n",
    "- is a sample-based estimate of the standard deviation of the irreducible error $\\epsilon$\n",
    "\n",
    "- provides a way to evaluate how well the model fits the data: the smaller the RSE, the better the model fits the data\n",
    "\n",
    "- $\\text{RSE} = \\sqrt{\\frac{1}{n - 2} \\text{RSS}} = \\sqrt{\\frac{1}{n - 2} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$\n",
    "\n",
    "    - where $\\text{RSS}$ is the residual sum of squares\n",
    "    \n",
    "    - the subtraction of 2 accounts for the two parameters $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ (reducing degrees of freedom by 2)\n",
    "\n",
    "    - this formula is - again - derived analytically\n",
    "\n",
    "    - it represents a standard deviation of the errors, because:\n",
    "        - RSS sums the squared *residuals*: this is analogous to how variance is calculated (summing squared deviations from the mean)\n",
    "        - dividing by $n - 2$ (degrees of freedom) is analogous to how variance divides by the number of independent observations\n",
    "        - taking the square root converts this into the same units as the original data, making it a standard deviation of the residuals\n",
    "\n",
    "- RSE is measured in the units of the target variable\n",
    "\n",
    "- in scikit-learn RSE is called `sklearn.metrics.root_mean_squared_error`\n",
    "    \n",
    "    - it doesn't account for the reduced degrees of freedom, because in practice with a large number n, the difference is neglible (statsmodels doesn't either)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R²$ statistic\n",
    "\n",
    "- is the proportion of total variance in $Y$ that is explained by the regression model\n",
    "\n",
    "- value is between 0 and 1\n",
    "\n",
    "- is independent of the scale of $Y$ (RSE is not)\n",
    "\n",
    "- $R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$\n",
    "\n",
    "    - where $\\text{TSS} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$ is the total sum of squares, if $X$ had no effect\n",
    "\n",
    "        - TSS measures the total variance in the response $Y$ , and can be thought of as the amount of variability inherent in the response before the regression is performed\n",
    "\n",
    "    - and $RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ is he residual sum of squares\n",
    "        - RSS measures the amount of variability that is left unexplained after performing the regression\n",
    "\n",
    "    - $\\text{TSS} - \\text{RSS}$ measures the amount of variability in the response that is explained (or removed) by performing the regression\n",
    "\n",
    "    - $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$\n",
    "\n",
    "- a score near 1 indicates a large proportion of variability explained by the model\n",
    "\n",
    "- a score near 0 indicates that the regression does not explain much of the variability in the response; this might occur because the linear model is wrong, or the error variance $\\sigma²$ is high, or both \n",
    "\n",
    "- like correlation, the $R²$ statistic is a measure of the linear relationship between $X$ and $Y$: </br>\n",
    "\n",
    "    $r = \\text{Cor}(X, Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}$\n",
    "\n",
    "    - where $\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$ is the **covariance** between $X$ and $Y$, which quantifies how $X$ and $Y$ vary together\n",
    "\n",
    "        - the covariance is not simply the product of both variances, but a measure of joint variability (how both random variables $X$ and $Y$ vary together *per pair*)\n",
    "\n",
    "    - and ${\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}$ is the product of the standard deviations of $X$ and $Y$, meaninh this term normalises the covariance\n",
    "\n",
    "    - it can be shown that in simple linear regression $R^2 = r^2$\n",
    "\n",
    "        - this doesn't scale to multiple linear regression though, since the correlation $r$ is a pairwise measure and can only be used on two random variables\n",
    "\n",
    "        - the $R²% statistic in contrast, is able to capture proportion of variance in the target that can be explained by the entire set of predictors (it doesn't account for possible multicollinearity though)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multiple Linear Regression\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit-learn_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
